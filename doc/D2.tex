%
% File acl2014.tex
%
% Contact: koller@ling.uni-potsdam.de, yusuke@nii.ac.jp
%%
%% Based on the style files for ACL-2013, which were, in turn,
%% Based on the style files for ACL-2012, which were, in turn,
%% based on the style files for ACL-2011, which were, in turn, 
%% based on the style files for ACL-2010, which were, in turn, 
%% based on the style files for ACL-IJCNLP-2009, which were, in turn,
%% based on the style files for EACL-2009 and IJCNLP-2008...

%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt]{article}
\usepackage{acl2014}
\usepackage{times}
\usepackage{url}
\usepackage{latexsym}

%\setlength\titlebox{5cm}

% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.


\title{LING 573 Deliverable \#1 Report}
\tiny
\author{Haobo Gu \\
  \small University of Washington \\
  {\tt haobogu@uw.edu} \\\And
 Yuanhe Tian \\
 \small University of Washington \\
  {\tt yhtian@uw.edu}\\\And
 Weifeng Jin \\
 \small University of Washington \\
  {\tt wjin@uw.edu} \\\And
 Haotian Zhu \\
 \small University of Washington \\
  {\tt haz060@uw.edu}\\}

\date{}

\begin{document}
\maketitle
\begin{abstract}

This document discusses the baseline automatic summarization system our team built for the Deliverable \#2 of LING 573 in Spring 2018. The system takes input as a set of document sets and generates the summarization for each document set respectively. Our system has four major parts: data preprocessing, content selection, information ordering and content realization. These four parts were implemented separately and assembled together for submission.
\end{abstract}

\section{Introduction}

Automatic summarization is a traditional natural language processing task, whose aim is to shorten the input text without losing too much information in the context and create a abbreviated, informative and consistent summary. Generally speaking, there are two widely-used approaches for this problem, extraction-based summarization and abstraction-based summarization. In this baseline system, our team built a system upon the extraction-based method, which extracts sentences from the input document set without modifying the content of the sentences. Currently, we used an unsupervised approach to assist with out extraction summary. Hence, there is no training and model construction required in this deliverable. Given the dataset documentation, our system can output the summary files individually without training process. We used \texttt{Python3} to implement our system.
\section{System Overview}

We employed the classic system structure mentioned in the lecture for the automatic summarization task, which consists three major parts, content selection, information ordering and content realization. Since the \texttt{AQUAINT} and \texttt{AQUAINT-2} Corpora have inconsistent data formatting, we add a data preprocessing part to prepare the data.  \\
\indent
The \texttt{data\_preprocessing} part takes one document specification (one \texttt{.xml} file) and the two corpora as input, and generate a \texttt{corpus} object, which includes \texttt{docsetList} and a token dictionary to record the frequency of each word occurrence in the input dataset. The \texttt{docsetList} object includes the document set ID, a token dictionary, a topic ID, a collection of summary texts generated by human beings and \texttt{documentCluster}. The \texttt{documentCluster} object is a list of document with its essential information, and each document has a list of sentences recorded.\\
\indent
The \texttt{content\_selection} part takes a document set as an input, and generates a list of sentences with their scores, computed by the LexRank algorithms. For each document set in the corpus, the part would generate a list of sentences individually.\\
\indent
The \texttt{information\_ordering} part takes a list of sentences, and returns a sorted list of sentences, using standards of chronology and other measures. \\
\indent
The \texttt{content\_realization} part takes the sorted list of sentences as input, and truncates the unnecessary sentences and low-rank sentences to generate an output summary for the corresponding document set. The output is a text file and for each document set in the corpus, the system would generate an individual summary text file for it.

\section{Approach}

As mentioned in the Introduction section, our system employs an unsupervised extraction approach and mainly uses the algorithms mentioned in the class. We used several different data structures, including list, dictionary and set to store the information extracted from the corpora and the detailed approach implementation for the three major parts would be discussed below and these approach is targeted to a single document set input.

\subsection{Content Selection}

Our team used the LexRank model to calculate the score for each sentence in the document set. Suppose we have $m$ sentences and $n$ tokens in the document set. According to the occurrence of the token, we can build a feature vector of length $n$ for each sentence and generate a $m\times n$ matrix for the document set, in which each row represents a sentence's feature vector. In addition to that, we compute the cosine distance for each sentence pair and generate a new $m\times m$ matrix, in which the $ij^{th}$ entry represents the cosine similarity between the $i^{th}$ sentence and the $j^{th}$ sentence. Next, we have to convert the matrix to be a transition probability matrix, in which each row of the matrix would be normalized, and mark this matrix as $M$. \\
\indent
Generate a 1-D vector $P$,  of length $m$ to record the score of each sentence, in which the $i^{th}$ entry represents the score for the $i^{th}$ sentence. The final step is trying to converge the vector $P$ using the update rule $P_t = M^T P_{t-1}$ to converge. In our experiment, we set the tolerance to be 0.001 and the convergence speed very fast.

\subsection{Information Ordering}

\subsection{Content Realization}
\section{Results}

This section contains the results of the deliverable project.

\section{Discussion}

This section contains the discussion of the deliverable project.

\section{Conclusion}

This section contains the conclusion of the deliverable project.

\begin{thebibliography}{}

\bibitem[\protect\citename{Jurafsky and Martin}2009]{Jurafsky:72}
Dan~Jurafsky and James~H. Martin.
\newblock 2009.
\newblock {\em Speech and language processing : an introduction to natural language processing, computational linguistics, and speech recognition}
\newblock Pearson Prentice-{Hall}, Upper Saddle River, NJ.

\bibitem[\protect\citename{{Manning and Schutze}}2000]{Manning:83}
{Christopher~D. Manning and Hinrich~Schutze}.
\newblock 2000.
\newblock {\em Foundations of statistical natural language processing}.
\newblock MIT Press, Cambridge, MA.

\end{thebibliography}

\end{document}
